El punto de partida es la [propuesta de tesis de Phillip Smith](https://www.cs.bham.ac.uk/~pxs697/publications/documents/rsmg3.pdf), que hace un análisis de sentimientos multivaluado. El enfoque propuesto por la misma es utilizar clustering para la clasificación. Difiere en nuestro objetivo en varios puntos clave: 
 * Es un solo lenguaje el que se analiza. 
 * Es un clasificador estático, lo cual implica que no importa cuando estén los resultados sino que estén.
 * Asume buena redacción/sintaxis, cosa no garantizable en Twitter.
 * Es siempre sobre notas de suicidio (tópico uniforme). 
 
En nuestro caso, el ambiente multilenguaje de por sí supone un nivel de datos muy grande como para poder manejarlos efectivamente mediante clustering; sin contar el hecho de que el volumen de features es exponencialmente grande y que vuelve la tarea mucho más complicada. Por estas razones se descarta ese enfoque a priori, pero se sigue considerando el análisis hecho en el paper ya que da un esquema útil para la definición de las clases de los sentimientos. 

Existen al menos dos cuestiones claves, el _qué_ y el _cómo_. No siempre es tan importante lo que se dice sino
también cómo se lo dice. Esto ayuda a distinguir ciertas subjetividades en el discurso. Por ejemplo, se 
puede enfatizar un estado de emoción violenta usando mayúsculas; la misma
frase en minúscula atenúa la intensidad de ese sentimiento puntual para el lector.
Además de las dos cuestiones mencionadas, existen otros mecanismos que le dan connotaciones
subjetivas a la misma construcción sintáctica. El con qué es parte de ello; el uso de emojis
pueden ser indicadores de ironía en una oración o restarle importancia a su significado limpio. _“Hoy me robaron”_ y
_“hoy me robaron =P”_ son drásticamente distintas. Este tipo de construcciones son dependientes del idioma,
el entorno sociocultural del que escribe y el público al que quiere dirigirse. En un ambiente multilenguaje resulta 
imposible de realizar a mano esta clasificación. En un entorno donde el espectro de emociones es más amplio y específico
apuntar simplemente a la sintaxis es una mala idea, aunque se puede asociar fácilmente un discurso 
a una escala de solo positivo-negativo. Cuando se intenta un mayor espectro de matices, resulta más difícil
lograr un buen resultado sin agregar otros indicadores. . Un claro ejemplo de esto es el uso de una lengua
extranjera: un estadounidense diciendo “_taco taco taco_” tiene una connotación distinta a si lo hace un
mexicano (es comúnmente usado como burla cuando alguien habla en español en un ambiente donde predomina el inglés). 

De esto se puede concluir que un enfoque clásico basado en lexicones no es un buen approach. Por un lado, porque hacerlo a mano es físicamente imposible; y por el otro, si se usan técnicas de construcción automática de lexicones se dejaría de lado todo el significado no semántico de cada palabra. En línea con esto, un enfoque orientado a información contextual, como los basados en word2vec y redes neuronales, se ven mucho más razonables. 

Dejando de lado el word2vec, los enfoques de redes neuronales tienen un cuello de botella muy importante que es la entrada de datos. Si uno no puede garantizar que un feature map sea acotado, no se puede crear y entrenar una red con ello. Por lo antes expuesto, si aceptamos todo como un feature, los feature maps son infinitos. Más aún, con el tiempo aparecen dinámicamente nuevas features a medida que aumenta el número de muestras disponibles. Claramente esto hace imposible hacer un sistema escalable, acorde a las necesidades usando este enfoque. A primera vista el hash trick sería una solución. La realidad es que al menos tiene un problema, para nosotros no menor: a medida que se agranda el universo de cosas que se pueden analizar (es un stream) llega indefectiblemente el momento en el que aparece algo que no se puede clasificar. Utilizando el hash trick, al perder información de qué feature es en el featuremap, perdemos la capacidad de saber eso de antemano, y más aún, puede matchear con algo que no corresponde. Descartando el hash trick terminamos en una situación en la cual no se puede avanzar dado el infinito número de features. Alternativas a eso serían utilizar clustering para matchear cada feature a un índice distinto de la entrada de la red, indicando la presencia de un grupo de features o no. Si bien esto parece una buena opción, el clustering clásico es lento, y agrupar según frecuencias de aparición en cada una de las clases supone un bias importante en la entrada de la red. Por todo esto es que word2vec sugiere ser una opción más útil para reducir las dimensiones a la entrada de la red. Aún así, a priori no soluciona el problema de las features infinitas, lo acota pero no lo resuelve por sí mismo. 

Aún así, con una pequeña modificación en la forma de trabajo se puede utilizar este mecanismo efectivamente. Word2vec cumple una doble función. Por un lado nos da una representación acotada de cada feature, con lo que el tamaño de la entrada se puede fijar en una de sus dos dimensiones (ancho). Por el otro, codifica dentro de esa representación, la información contextual del feature. Estas dos características son ampliamente buscadas para el enfoque a adoptar. Aún así, por si solo sigue sin ser la solución alternativa al problema de las features infinitas. Recordando que se busca usar features que van más allá del simple significado de la palabra, aún se sigue sin poder acotar el número total de features a la entrada. Más aún, suma un nuevo problema. Si se plantea utilizar el mismo word2vec para codificar palabra y otras cosas, por un lado el algoritmo tardaría demasiado y por el otro encontraría relaciones que son inútiles y pueden afectar al resultado final. Por ejemplo, siempre que se encuentre HOLA, se va a encontrar hola (suponiendo 2 tokenizers, uno sin preprocesamiento y otro que pasa todo a minúsculas), ahora, es claro que hola todo en mayúsculas no está necesariamente relacionado con el significado de hola. 

Aún así, se puede pensar lo que siguiente: en un tweet no puede haber más de 70 palabras. Las palabras en sí codifican el sentido semántico del tweet. Las palabras y el idioma de origen del tweet, las palabras y si son hashtags o no y demases, codifican la información meta sintáctica del tweet. La cantidad de cada tipo de feature está completamente acotada dada la longitud máxima de un tweet. Se puede entonces separar el embeding de cada grupo de features según su rol. De esta forma, los espacios quedan más chicos y las features tienen real relación entre sí. Así también, se puede determinar la cantidad máxima de features de cada tipo que pueden aparecer, acotando finalmente el feature map. 

Con esto, se pueden agregar incluso otras features de soporte, por ejemplo embeddings a nivel caracter. Aún cuando se demostró que por sí mismos los embeddings a nivel caracter no daban buenos resultados, se los puede tener como información de respaldo. En un contexto donde la sintaxis está lejos de ser perfecta, se puede utilizar cierta información de los caracteres utilizados para aproximar a la clasificación esperada. 

Con esta metodologia, se puede utilizar un conjunto arbitrario de tokenizers y siempre tener un feature map de tamaño fijo y un embeding representativo de cada feature generada. Para poder explotar word2vec de esta forma, es necesario poder representar cada feature como una cadena de forma inequívoca dentro de su espacio de features. El resto es simplemente una variación de ajustes de los metaparametros del word2vec, como por ejemplo el tamaño de la ventana. 

A nivel implementación, cada conjunto de features debería realizarse, embeberlas en workers separados, y el resultado de cada uno volcarse a un archivo “distribuido” el cual debe ser unido antes de pasarselo a la red. 

En cuanto al nivel de entrada se puede incrementar el nivel de tweets a la entrada utilizando workers distribuidos como entrada. Usando RabbitQ, se puede distribuir una aplicación que permita a varios usuarios contribuir con el caudal de datos. Incluso hacer eso permite segmentar el área desde donde se consiguen los tweets, logrando así una entrada mucho más uniforme para cada país. Esto suplementaria la falta de acceso a Firehose.
